{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline  \n",
    "\n",
    "import pandas as pd\n",
    "import consolidateFiles as cf\n",
    "import datacleaning as cl\n",
    "import fragmentation as fr\n",
    "import hervpd as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ! Parse activity files and parse interval files are to be replaced with the corresponding database queries as soon as they are available "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Pipeline configuration \n",
    "* set the input/output directories, user id and verbose level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "PATH = \"C:\\\\Users\\\\ju\\\\GDrive\\\\Projects\\\\HeRV\\\\Data\\\\\"\n",
    "RAW_PATH = PATH + \"Raw\"\n",
    "PRE_PATH = PATH + \"PreProcessed\"\n",
    "\n",
    "sessfile = PRE_PATH + \"\\\\sessions.xlsx\"\n",
    "    \n",
    "# duration (in seconds) to be cropped from the beginning of each second to account for stabilization and user adjustment\n",
    "crop = 90\n",
    "    \n",
    "# duration (in seconds) of each fragment to be sent to analysis\n",
    "duration = 300\n",
    "    \n",
    "# if any fragment has more than 'threshold' consecutive seconds with no beats, it will be discarded\n",
    "threshold = 3    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Read sessions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(sessfile)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = df.to_dict(orient='records')\n",
    "print(sessions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Generate fragments from sessions\n",
    "\n",
    "Breaks the sessions duration in fragments\n",
    "\n",
    "Configurations:\n",
    "* duration of each fragment in seconds;\n",
    "* number of seconds to be discarded at the beginning of the session, accounting for user's stabilization and adjustment to posture and activity \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frags = cf.fragment_sessions(sessions, duration, crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(frags))\n",
    "print(frags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Add and clean interval data to fragments\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Extract beats in fragment\n",
    "\n",
    "Retrieves from the heartbeat files all the intervals contained in each session's duration and adds them to the fragments objects (in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragall(frags, path):\n",
    "    for i, f in enumerate(frags):\n",
    "        if (i % 100 == 0):\n",
    "            print (i, '/', len(frags))\n",
    "        f['rr'] = cf.beats_in_fragment(f, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time fragall(frags, RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(frags)\n",
    "df['beatcount'] = df['rr'].apply(len)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Remove outliers from RR series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rr'] = df['rr'].apply(cl.clean_rr_series)\n",
    "df['beatcount'] = df['rr'].apply(len)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Remove fragments with too few beats (due to hardware malfunction or software was not recording beats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['beatcount'] > 0.6 * duration]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Extract time and frequency domain features\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TODO*** do it using apply: df[feature_list] = df.apply(lambda row: pd.Series(aggregate_function(row['rr']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = df.to_dict(orient='records')\n",
    "for i in dic:\n",
    "    i.update(cf.features_from_dic(i['rr']))\n",
    "print(dic[0]['rmssd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dic)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing HF outliers caused by small gaps between the recorded intervals, to which HF is particularly sensitive\n",
    "\n",
    "***TODO*** it is best to actually remove the cause by separating continuous sequences in the interval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = df[df['hf'] < 7000]\n",
    "print(len(df), 'original and', len(dfr), 'after pruning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Save\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = dfr.drop(['rr'], axis = 1)\n",
    "df_output.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = PRE_PATH + '\\\\df_' + str(duration) + '_' + str(crop) + '.xlsx'\n",
    "print(filename)\n",
    "df_output.to_excel(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Save LDA Grover\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr[['activity']].to_csv('./classifications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ints(beats):\n",
    "    return [beat['interval'] for beat in beats]\n",
    "\n",
    "dfr['ts'] = dfr['rr'].apply(get_ints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr.loc[['ts']].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr[['ts']].to_csv('./timeseries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr[cl.features_all].to_csv('./features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in d.activity.unique()\n",
    "       df.groupby(column).count()['user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying all steps above to generate different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#durations = [300, 240, 180, 150, 120, 90, 60]\n",
    "#crops = [120, 90, 60]\n",
    "\n",
    "durations = [450, 600]\n",
    "crops = [90, 60]\n",
    "\n",
    "def multifrag(sessions, durations, crops, path_in, path_out):\n",
    "    for cr in crops:\n",
    "        for dr in durations:\n",
    "            fname = path_out + '\\\\df_' + str(dr) + '_' + str(cr) + '.xlsx'\n",
    "            print ('generating', fname, '...')\n",
    "            ds = fr.gen_fragments_dataset(sessions, dr, cr, path_in)\n",
    "            print('resulting dataset:', len(ds), 'records' )\n",
    "            ds.to_excel(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time multifrag(sessions, durations, crops, RAW_PATH, PRE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
